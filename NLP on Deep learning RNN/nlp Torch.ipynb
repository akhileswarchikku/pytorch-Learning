{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c81a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0990a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5a6f159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5445609\n"
     ]
    }
   ],
   "source": [
    "with open(r\"D:\\MegaSync\\MEGAsync\\MegaSync\\Machine Learning Projects\\pytoch_learnings\\PYTORCH_NOTEBOOKS\\Data\\shakespeare.txt\",\"r\",encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(len(text))\n",
    "    import re\n",
    "    text = re.sub(' +',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c62bf132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5053317\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68376857",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee8ad974",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d5376e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e10a5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b097f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text,num_uni_chars):\n",
    "    one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "23f697f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1,2,0])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2dd40ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d6eeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text,samp_per_batch=10,seq_len=50):\n",
    "    char_per_batch =samp_per_batch * seq_len\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch,-1))\n",
    "    \n",
    "    for n in range(0,encoded_text.shape[1],seq_len):\n",
    "        x = encoded_text[:,n:n+seq_len]\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,n+seq_len]\n",
    "        \n",
    "        except:\n",
    "            y[:,:-1] = x[:,1:]\n",
    "            y[:,-1] = encoded_text[:,0]\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe34e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75effdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(encoded_text=sample_text,samp_per_batch=2,seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "369ee093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42 38 18 42 38]\n",
      " [ 9 40 45 68 63]]\n",
      "[[ 4 68 41 64 38]\n",
      " [65  0 38 47 68]]\n"
     ]
    }
   ],
   "source": [
    "for i in batch_generator:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04792eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 38, 18, 42, 38,  4, 68, 41, 64, 38,  9, 40, 45, 68, 63, 65,  0,\n",
       "       38, 47, 68])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3dde8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,all_chars,num_hidden=256,num_layers=4,drop_prob=0.5,use_gpu=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char:ind for ind,char in decoder.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars),num_hidden,num_layers,dropout=drop_prob,batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden,len(self.all_chars))\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        \n",
    "        lstm_output,hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1,self.num_hidden)\n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "        \n",
    "    def hidden_state(self,batch_size):\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                      torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                      torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06914c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =CharModel(all_chars=all_characters,\n",
    "                 num_hidden=512,\n",
    "                 num_layers=3,\n",
    "                 drop_prob=0.5,\n",
    "                 use_gpu=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "863cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_param = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "067e428c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74d858d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "181e9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "27bacc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text)*train_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6949fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed44a145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505331"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "794ebb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4547986"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4576d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bda106f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae67f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 100\n",
    "tracker = 0\n",
    "seq_len = 100\n",
    "\n",
    "num_char = max(encoded_text)+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54577fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.300767660140991\n",
      "Epoch: 0 Step: 50 Val Loss: 3.2924044132232666\n"
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7aabc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hidden512_layers3_shakes.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "758d1bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(),model_name)\n",
    "model.load_state_dict(torch.load(r\"D:\\MegaSync\\MEGAsync\\MegaSync\\Machine Learning Projects\\pytoch_learnings\\PYTORCH_NOTEBOOKS\\06-NLP-with-PyTorch\\Final_Shakespeare.net\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "975a86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        # Encode raw letters with model\n",
    "        encoded_text = model.encoder[char]\n",
    "        \n",
    "        # set as numpy array for one hot encoding\n",
    "        # NOTE THE [[ ]] dimensions!!\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        \n",
    "        # One hot encoding\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        \n",
    "        # Grab hidden states\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "        \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "        \n",
    "        \n",
    "        # k determines how many characters to consider\n",
    "        # for our probability choice.\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "        \n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        \n",
    "        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().flatten()\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf2bb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "      \n",
    "    \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "565308b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my name isl]OJ)7JPJX)\"s'l>e}JJJJG9sJKl\"\"JPJXN)> JN)JX)JX)JX)Jw)>lJ)7JOl>JXllwe}JJJJ?OPNJNO)pJP>NJXll9JNOPNJXO)p\"sJXN>P9zlJN>P'wl9NX}JJJJ?OPNJNO)pJXP JXOlJXl9sJNOlJXp9J)7JNOllB}JJJJ?OlJw)NOl>J)7JNOlJXNP>XJ)7JNO'XJK)>sXJK'NO}JJJJGJXl\"7J)7JOPNlJ)7Jw JOP9sB}JJvR[&SG?(GBJHXJO'wJX)J\")g'9zJNOPNJHJO)pXle}JJJJ?)JNO'XJ'9JNOlJ])p\"NJ)7JOlP>NJN)JNOlJ])9X'sl>N}JJJJGXJHJK)p\"sJXNP JNOlJK'9sJPNJNOlJ])9Nl9NB}}JJJJJJJJJJJJJJJJJJJJJ[9Nl>JGQ?&QC}}JJvR[&SG?(GBJaOPNJK'NOJNO JXl\"7JKOl>lJ'XJw }JJJJ?)JNOlJXp>XJ)7JN'wlXJP9sJX)9JNOlJK)>\"sJIl}JJJJ?)JNOlJN)JOPNlJN)JNOlJ7P>NOJ)7JNOlJXN>'9zJ'9N)}JJJJG9sJI>lPNOl>J)7JwlB}JJGM(HSSGBJal\"\"eJwPsPweJX'>eJHJKPXJNO'9VJ)7JNOlJK)>\"se}JJJJ?OPNJNO)pJP>NJXl>g.sJN)JwPVlJwlJNOlJXp]OJPJX)\"s}JJJJ?OP9JXOlJK'NOJNO'XJK)>NOJN>P'X)9XJ)9JNOlJ]P>lB}JJJJHJO)pXNJNO)pJOPsJOl>JX)9JN'\"\"JOlJXN'\"\"JIlJX)9e}JJJJ?OPNJNO)pJP>NJNO)pJP>NJPJw)>lB}JJv_G(yHGQBJHJOPglJ9)NJXl>g'9ze}JJJJG9sJN)JNOlJKPXNlJ)7JOl>JK)>sJNOP9JXOlJK'NOJNOllB}JJJJH7JNO)pJXO)p\"sXNJXNP9sJOl>JXOPwleJNOlJK'7lJ)pN}JJJJa'NOJP\"\"JNOlJXNPNle}JJJJG9sJK'NO\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='Hi my name is', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12a1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
